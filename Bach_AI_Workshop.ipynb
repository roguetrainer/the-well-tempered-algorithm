{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \ud83c\udfb9 The Well-Tempered Algorithm\n",
    "## Learning AI Through the Music of Bach\n",
    "\n",
    "Welcome to this interactive workshop. We will progress through three distinct eras of AI music generation:\n",
    "\n",
    "1.  **The Probabilistic Era:** Using **Markov Chains** to understand style as a game of dice.\n",
    "2.  **The Deep Learning Era:** Using **LSTMs** (Recurrent Neural Networks) to gain \"memory.\"\n",
    "3.  **The Transformer Era:** Using **GPT-2** architectures to treat music as a language.\n",
    "\n",
    "### Prerequisites\n",
    "We need a few libraries. `music21` is our toolkit for handling sheet music data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install music21 tensorflow transformers numpy\n",
    "\n",
    "import music21\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Activation\n",
    "from transformers import TFGPT2LMHeadModel, GPT2Config\n",
    "\n",
    "print(\"Environment Ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## Module 1: The Markov Chain (Probability)\n",
    "\n",
    "Before we use heavy AI, let's understand the basics. A Markov chain generates music by asking: *\"Given the note I just played, what is the most likely note to follow?\"*\n",
    "\n",
    "It has no memory of the past beyond the current moment. It is perfect for capturing the **texture** of Bach, but terrible at **structure**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Load Data ---\n",
    "def get_bach_chorale():\n",
    "    # We load a specific chorale from the music21 corpus\n",
    "    print(\"Loading Bach Chorale BWV 66.6...\")\n",
    "    return music21.corpus.parse('bach/bwv66.6')\n",
    "\n",
    "# --- 2. Train (Build Dictionary) ---\n",
    "def train_markov_chain(score):\n",
    "    soprano = score.parts[0].flatten().notes\n",
    "    transitions = {}\n",
    "    \n",
    "    for i in range(len(soprano) - 1):\n",
    "        curr = soprano[i].nameWithOctave\n",
    "        next_n = soprano[i+1].nameWithOctave\n",
    "        \n",
    "        if curr not in transitions:\n",
    "            transitions[curr] = []\n",
    "        transitions[curr].append(next_n)\n",
    "    return transitions\n",
    "\n",
    "# --- 3. Generate ---\n",
    "def generate_markov(chain, length=20):\n",
    "    current = random.choice(list(chain.keys()))\n",
    "    melody = [current]\n",
    "    \n",
    "    for _ in range(length):\n",
    "        if current in chain:\n",
    "            current = random.choice(chain[current])\n",
    "            melody.append(current)\n",
    "        else:\n",
    "            break\n",
    "    return melody\n",
    "\n",
    "# --- Run Module 1 ---\n",
    "score = get_bach_chorale()\n",
    "brain = train_markov_chain(score)\n",
    "new_melody = generate_markov(brain)\n",
    "print(\"Markov Melody:\", new_melody)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## Module 2: The LSTM (Long Short-Term Memory)\n",
    "\n",
    "Markov chains forget instantly. An **LSTM** is a Neural Network designed to have a \"conveyor belt\" of memory. It looks at a *sequence* of notes (e.g., the last 10) to decide the next one.\n",
    "\n",
    "**Note:** Deep Learning requires data. We will load 10 chorales. In a real scenario, you would want 300+."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Data Prep ---\n",
    "def get_lstm_data():\n",
    "    chorales = music21.corpus.getComposer('bach')[:10]\n",
    "    all_notes = []\n",
    "    for c in chorales:\n",
    "        try:\n",
    "            parsed = music21.corpus.parse(c)\n",
    "            notes = parsed.parts[0].flatten().notes\n",
    "            for n in notes:\n",
    "                if isinstance(n, music21.note.Note):\n",
    "                    all_notes.append(n.nameWithOctave)\n",
    "                elif isinstance(n, music21.chord.Chord):\n",
    "                    all_notes.append(n.notes[-1].nameWithOctave)\n",
    "        except:\n",
    "            continue\n",
    "    return all_notes\n",
    "\n",
    "raw_notes = get_lstm_data()\n",
    "pitchnames = sorted(set(raw_notes))\n",
    "note_to_int = {n: i for i, n in enumerate(pitchnames)}\n",
    "int_to_note = {i: n for i, n in enumerate(pitchnames)}\n",
    "n_vocab = len(pitchnames)\n",
    "\n",
    "# Create Sequences\n",
    "SEQ_LEN = 10\n",
    "network_in = []\n",
    "network_out = []\n",
    "\n",
    "for i in range(len(raw_notes) - SEQ_LEN):\n",
    "    seq_in = raw_notes[i:i + SEQ_LEN]\n",
    "    seq_out = raw_notes[i + SEQ_LEN]\n",
    "    network_in.append([note_to_int[c] for c in seq_in])\n",
    "    network_out.append(note_to_int[seq_out])\n",
    "\n",
    "X = np.reshape(network_in, (len(network_in), SEQ_LEN, 1)) / float(n_vocab)\n",
    "y = tf.keras.utils.to_categorical(network_out)\n",
    "\n",
    "# --- 2. Model ---\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(LSTM(128, input_shape=(X.shape[1], X.shape[2])))\n",
    "model_lstm.add(Dense(n_vocab))\n",
    "model_lstm.add(Activation('softmax'))\n",
    "model_lstm.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "# --- 3. Train (Short run for demo) ---\n",
    "print(\"Training LSTM...\")\n",
    "model_lstm.fit(X, y, epochs=5, batch_size=64)\n",
    "\n",
    "# --- 4. Generate ---\n",
    "start = np.random.randint(0, len(network_in)-1)\n",
    "pattern = network_in[start]\n",
    "prediction_output = []\n",
    "\n",
    "for i in range(20):\n",
    "    input_x = np.reshape(pattern, (1, len(pattern), 1)) / float(n_vocab)\n",
    "    prediction = model_lstm.predict(input_x, verbose=0)\n",
    "    index = np.argmax(prediction)\n",
    "    result = int_to_note[index]\n",
    "    prediction_output.append(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:]\n",
    "\n",
    "print(\"LSTM Melody:\", prediction_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## Module 3: The Transformer (GPT-2)\n",
    "\n",
    "Modern AI treats music as a **Language**. We will use Hugging Face's GPT-2 architecture. Instead of reading left-to-right like an LSTM, the Transformer uses **Self-Attention** to see the whole musical phrase at once.\n",
    "\n",
    "We tokenize music into strings: `\"C#4_1.0\"` (Pitch_Duration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Tokenization ---\n",
    "dataset_strings = []\n",
    "vocab = set()\n",
    "\n",
    "# We reuse the raw data logic but include duration\n",
    "for c in music21.corpus.getComposer('bach')[:5]:\n",
    "    try:\n",
    "        s = music21.corpus.parse(c)\n",
    "        flat = s.parts[0].flatten().notes\n",
    "        tokens = []\n",
    "        for n in flat:\n",
    "            if isinstance(n, music21.note.Note):\n",
    "                tok = f\"{n.nameWithOctave}_{n.quarterLength}\"\n",
    "                tokens.append(tok)\n",
    "                vocab.add(tok)\n",
    "        dataset_strings.append(tokens)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "vocab_list = sorted(list(vocab))\n",
    "t2i = {t: i for i, t in enumerate(vocab_list)}\n",
    "i2t = {i: t for i, t in enumerate(vocab_list)}\n",
    "\n",
    "# --- 2. GPT Model Config ---\n",
    "config = GPT2Config(\n",
    "    vocab_size=len(vocab),\n",
    "    n_positions=32,\n",
    "    n_ctx=32,\n",
    "    n_embd=128,\n",
    "    n_layer=2,\n",
    "    n_head=4\n",
    ")\n",
    "gpt_model = TFGPT2LMHeadModel(config)\n",
    "gpt_model.build(input_shape=(None, 32))\n",
    "gpt_model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
    "print(\"GPT Model initialized.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}